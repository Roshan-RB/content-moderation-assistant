# ğŸ›¡ï¸ Content Moderation Assistant using LangChain & Streamlit

This is a simple but production-inspired AI app that simulates how content moderation systems can classify and respond to user comments. The app uses **LangChain's conditional chains** to classify a given comment into categories like spam, abusive, constructive, or off-topic â€” and generate appropriate moderator responses automatically.

Built with:

* ğŸ’¬ **LangChain** for prompt orchestration and logic flow
* ğŸ§ **OpenAI (GPT)** for classification and response generation
* ğŸ›ï¸ **Streamlit** for a clean and interactive frontend

---

## ğŸš€ Features

* **Comment Classification**: Categorizes user input into one of: `spam`, `abusive`, `constructive`, or `off-topic`.
* **Conditional Reply Generation**: Automatically generates a polite or firm moderator response based on the classification.
* **Realistic Examples**: Try out pre-filled realistic comments to see how the system behaves.
* **Educational Use**: Great for learning LangChain branching (`RunnableBranch`) and prompt engineering.

---

## ğŸ”§ How It Works

### ğŸ” LangChain Conditional Chaining

This project uses LangChain's `RunnableBranch` to conditionally route logic:

1. **Classification Chain**:

   * Uses a prompt + OpenAI model to determine the type of comment
   * Output is parsed using `PydanticOutputParser`

2. **Conditional Branching**:

   * Based on the output class (`spam`, `abusive`, etc.), the flow branches to a different prompt template that generates a custom reply

```python
if comment_type == "spam":
    reply = spam_reply_chain.run(comment)
elif comment_type == "abusive":
    reply = abusive_reply_chain.run(comment)
...
```

âœ… All done declaratively using LangChain with reusable blocks.

---

## ğŸ§  Why This Matters

Though simple, this workflow reflects real-world moderation pipelines used by:

* ğŸ¥ Social platforms (Reddit, YouTube, Discord, etc.)
* ğŸ›ï¸ E-commerce review managers
* ğŸ—£ï¸ Customer support ticket responders
* ğŸ§‘â€âš–ï¸ Automated complaint handlers

With minor extensions, this can evolve into a **moderation agent system**:

* Flag â†’ Route â†’ Respond â†’ Escalate
* Store feedback or explanation
* Notify human-in-the-loop moderator

---

## ğŸ› ï¸ How to Run Locally

```bash
git clone https://github.com/yourusername/content-moderation-assistant.git
cd content-moderation-assistant

# (Optional) Create a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Add your OpenAI API key
echo "OPENAI_API_KEY=your-key-here" > .env

# Run the app
streamlit run app.py
```

---

## ğŸ“ Project Structure

```
content-moderation-assistant/
â”‚
â”œâ”€â”€ app.py                 # Streamlit frontend
â”œâ”€â”€ chains.py              # LangChain logic (classification + branching)
â”œâ”€â”€ prompts/               # Prompt templates
â”‚   â”œâ”€â”€ classify_prompt.txt
â”‚   â”œâ”€â”€ spam_reply.txt
â”‚   â”œâ”€â”€ abusive_reply.txt
â”‚   â”œâ”€â”€ constructive_reply.txt
â”‚   â””â”€â”€ offtopic_reply.txt
â”œâ”€â”€ .env                   # API Key (not committed)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”® Future Ideas

* Add **feedback buttons**: "Was this classification helpful?"
* Show **explanation / chain-of-thought** reasoning
* Extend to support **multilingual moderation**
* Integrate with **LangGraph** or **Agents** for escalation workflows
* Deploy to Streamlit Cloud, Hugging Face Spaces, or Docker

---


